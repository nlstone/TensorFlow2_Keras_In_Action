{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")  \n",
    "for gpu in gpus:\n",
    "    # 设置内存增长方式 自增长\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../datasets/shakespeare/shakespeare.txt'\n",
    "text = open(file_path, 'r', encoding='utf-8').read()\n",
    "\n",
    "print('text length:', len(text))\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成词表 char -> id\n",
    "vocab = sorted( set(text) )\n",
    "print(len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_id = {char:id for id, char in enumerate(vocab)}\n",
    "print(char_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_of_text = np.array( [char_to_id[char] for char in text] )\n",
    "print(text[:20])\n",
    "print(ids_of_text[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(text_ids):\n",
    "    # [ 1 2 3 4 5] -> [1 2 3 4], [2 3 4 5]\n",
    "    return text_ids[:-1], text_ids[1:]\n",
    "seq_length = 100\n",
    "seq_num = len(ids_of_text) // seq_length\n",
    "ids_of_text = ids_of_text[:seq_length * seq_num]\n",
    "print( ids_of_text.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_of_text = ids_of_text.reshape(-1, seq_length)\n",
    "print(ids_of_text.shape)\n",
    "print(ids_of_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_ids = map(split_input_target, ids_of_text)\n",
    "seq_ids = np.array(list(seq_ids))\n",
    "print(seq_ids.shape)\n",
    "print(seq_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array( [item[0] for item in seq_ids] )\n",
    "train_label = np.array( [item[1] for item in seq_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rnn model\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "rnn_units = 1024\n",
    "batch_size = 64\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                           batch_input_shape = [batch_size, None]),\n",
    "    keras.layers.GRU(units = rnn_units, stateful = True, return_sequences = True),\n",
    "    keras.layers.Dense(vocab_size, activation = 'softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = train_data.shape[0] // batch_size\n",
    "train_data = train_data[:step * batch_size]\n",
    "train_label = train_label[:step * batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "callbacks_dir = './callbacks'\n",
    "if not os.path.exists(callbacks_dir):\n",
    "    os.makedirs(callbacks_dir)\n",
    "best_model_file_path = os.path.join(callbacks_dir, 'best_text_generate_model.h5')\n",
    "# load saved model\n",
    "\n",
    "try:\n",
    "    model.load_weights(best_model_file_path)\n",
    "    print('Load weights suc! Continue to fit model.')\n",
    "except:\n",
    "    print('Load weights failed! Start to fit new model.')\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(min_delta=1e-3, patience=10, monitor='loss'),\n",
    "    keras.callbacks.ModelCheckpoint(best_model_file_path, save_best_only=True, monitor='loss')  \n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, train_label,\n",
    "                    epochs = 20,\n",
    "                    batch_size = batch_size,\n",
    "                    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(train_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_label[0])\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_batch_size = 1\n",
    "single_batch_model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                           batch_input_shape = [single_batch_size, None]),\n",
    "    keras.layers.GRU(units = rnn_units, stateful = True, return_sequences = True),\n",
    "    keras.layers.Dense(vocab_size, activation = 'softmax')\n",
    "])\n",
    "single_batch_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_batch_model.load_weights(best_model_file_path)\n",
    "single_batch_model.build(input_shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = single_batch_model.predict_classes(train_data[0:1], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 44, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 13, 43, 1, 53, 56, 43, 1, 58, 43, 1, 46, 56, 53, 60, 43, 43, 42, 10, 39, 52, 42, 1, 47, 59, 56, 58, 46, 43, 56, 8, 1, 58, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 15, 50, 50, 10, 0, 31, 54, 43, 39, 49, 1, 1, 57, 54, 43, 39, 49, 1, 0, 0, 24, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 13, 53, 59]\n",
      "[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n",
      " 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
      " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n",
      "  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n",
      " 37 53 59]\n"
     ]
    }
   ],
   "source": [
    "print(pred)\n",
    "print(train_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "----------\n",
      "Rfst Citizen:\n",
      "Ae ore te hroveed:and iurther. tear me speak.\n",
      "\n",
      "Cll:\n",
      "Speak  speak \n",
      "\n",
      "Lirst Citizen:\n",
      "Aou\n"
     ]
    }
   ],
   "source": [
    "print(''.join(vocab[id] for id in train_label[0]))\n",
    "print('----------')\n",
    "print(''.join(vocab[id] for id in pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "\n",
      "GLOUCESTER:\n",
      "And so was ever to be spice,\n",
      "One rancour ood and that I should bound to sea:\n",
      "I am that prize burntly, fair benefit\n",
      "As have him sours instantly, good friar, be not sweet.\n",
      "\n",
      "JOHN OF GAUNT:\n",
      "Cousin, farewell, e got, not I.\n",
      "\n",
      "Second Murderer:\n",
      "And now I stay, they shall merry man? thy cabinit\n",
      "By Aulivily, and himself; and tell him where\n",
      "God you look you from af any ora,\n",
      "Come homey with him, Aufidius, with his schole,\n",
      "As you have made thee ach-own proceedings in the track'd\n",
      "Whereof these fi\n"
     ]
    }
   ],
   "source": [
    "# 文本生成，每次生成一个字符，并对结果进行抽样（否则结果是固定的，容易陷入循环）\n",
    "def generate_text(model, start_string, num_generate = 500):\n",
    "    input_data = [char_to_id[char] for char in start_string]\n",
    "    input_data = tf.expand_dims(input_data, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    for _ in range(num_generate):\n",
    "        # 模型前向传播 -> 推测出下一个字符（可能性）\n",
    "        # -> 抽样 -> 生成下一个字符 -> 更新输入数据，准备下一次前向传播\n",
    "        predictions = model(input_data)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        prediction = predictions[-1]\n",
    "        \n",
    "        # 抽样\n",
    "        prediction = prediction.numpy() #[0.01, 0.94...]\n",
    "        predicted_id = np.random.choice(range(len(prediction)), 1, p = prediction)[0]\n",
    "        \n",
    "        #生成下个字符\n",
    "        text_generated.append( vocab[predicted_id] )\n",
    "        \n",
    "        #更新输入数据，准备下一次前向传播\n",
    "        input_data = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "new_text = generate_text(single_batch_model, \"All:\")\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
